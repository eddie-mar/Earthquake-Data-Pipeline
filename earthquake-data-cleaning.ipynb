{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bd84a4f6-0448-4f27-9165-f278841b9c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "\n",
    "from datetime import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, from_unixtime, to_timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3422737a-dbb7-42ca-b8e5-723a3e0e49fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/07/26 11:44:42 WARN Utils: Your hostname, recurSe resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)\n",
      "25/07/26 11:44:43 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/07/26 11:44:43 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName('earthquake-data-cleaning').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ebad4018-909b-4d93-8ac3-14d526d7fcc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "df = spark.read.option('header', 'true').option('inferSchema', 'true').csv('earthquake-data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd1051f2-48af-41a0-8aaa-0182ebb2adfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+---------+----------+------------+-----+-----+-------+----+----------+\n",
      "|               place|         time|magnitude|       lat|        long|depth|alert|tsunami|  tz|      type|\n",
      "+--------------------+-------------+---------+----------+------------+-----+-----+-------+----+----------+\n",
      "|8km SSW of Lytle ...|-631157391770|     2.58|34.1911667|    -117.522| 4.49| null|      0|null|earthquake|\n",
      "|24km WNW of Searl...|-631215832260|     2.01|35.8593333|-117.6506667|  0.0| null|      0|null|earthquake|\n",
      "|28km N of El Sauz...|-631241139690|      3.3|32.1433333|-116.6288333|  6.0| null|      0|null|earthquake|\n",
      "|1km SSW of Artesi...|-631251141040|     1.83|33.8561667|-118.0893333| 0.25| null|      0|null|earthquake|\n",
      "|16km SE of Primo ...|-631284369930|     3.02|    32.113|-116.8063333|  6.0| null|      0|null|earthquake|\n",
      "|south of the Fiji...|-631286334600|     null|   -26.927|    -176.566| 15.0| null|      0|null|earthquake|\n",
      "|10 km SSW of Sawa...|-631292588380|     null|    35.793|     140.453| 35.0| null|      0|null|earthquake|\n",
      "|91 km NNE of Lase...|-631331600890|     null|    -5.923|     111.759|605.0| null|      0|null|earthquake|\n",
      "|Kermadec Islands ...|-631351025130|     null|   -27.889|    -177.108|135.0| null|      0|null|earthquake|\n",
      "|1km SW of Westwoo...|-631369460570|     1.82|34.0471667|   -118.4385|  6.0| null|      0|null|earthquake|\n",
      "|2 km SSE of Furao...|-631374120000|     null|    17.017|     121.808| 35.0| null|      0|null|earthquake|\n",
      "|15 km NE of Siman...|-631388218290|     null|     17.58|     121.946| 35.0| null|      0|null|earthquake|\n",
      "|7 km N of Agbanna...|-631400163400|     7.21|    17.439|     121.538| 15.0| null|      0|null|earthquake|\n",
      "|21km SW of Little...|-631402302400|     2.03|35.7918333|     -118.05|  6.0| null|      0|null|earthquake|\n",
      "|33 km SSE of Adak...|-631448307640|     null|    51.584|    -176.509| 35.0| null|      0|null|earthquake|\n",
      "|east of the South...|-631460084000|     null|   -59.169|     -20.328| 15.0| null|      0|null|earthquake|\n",
      "|174 km NW of Sant...|-631474473850|     null|    40.271|     -29.346| 15.0| null|      0|null|earthquake|\n",
      "|162 km ENE of San...|-631483741940|     null|    40.191|     -29.486| 15.0| null|      0|null|earthquake|\n",
      "|east of the South...|-631497764890|     6.68|   -60.052|     -21.755| 10.0| null|      0|null|earthquake|\n",
      "|193 km WNW of Asa...|-631508164560|     null|   -13.124|     -174.38| 35.0| null|      0|null|earthquake|\n",
      "+--------------------+-------------+---------+----------+------------+-----+-----+-------+----+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b46e4e64-8ea6-4b0b-8674-e109d7fa4ca2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- place: string (nullable = true)\n",
      " |-- time: long (nullable = true)\n",
      " |-- magnitude: double (nullable = true)\n",
      " |-- lat: double (nullable = true)\n",
      " |-- long: double (nullable = true)\n",
      " |-- depth: double (nullable = true)\n",
      " |-- alert: string (nullable = true)\n",
      " |-- tsunami: integer (nullable = true)\n",
      " |-- tz: double (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9872ecdb-7402-4026-b199-0e86fe0deaa4",
   "metadata": {},
   "source": [
    "I have targetted the data cleaning procedures needed base on the data exploration I've done in pandas. First, let's convert the time column into a useful and readable format. Also, let's check if there are no invalid time or out of range time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "876fd8a2-4597-4650-b52b-04b408563702",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fmt_time = df.withColumn('earthquake_datetime', from_unixtime(col('time')/1000)). \\\n",
    "    withColumn('earthquake_datetime', to_timestamp('earthquake_datetime'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "82746f1f-1a6a-4b66-95cf-d23597f380dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- place: string (nullable = true)\n",
      " |-- time: long (nullable = true)\n",
      " |-- magnitude: double (nullable = true)\n",
      " |-- lat: double (nullable = true)\n",
      " |-- long: double (nullable = true)\n",
      " |-- depth: double (nullable = true)\n",
      " |-- alert: string (nullable = true)\n",
      " |-- tsunami: integer (nullable = true)\n",
      " |-- tz: double (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- earthquake_datetime: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_fmt_time.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "99162058-3ad1-4050-8db6-7aec814b6093",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "data": {
      "text/plain": [
       "4608354"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_fmt_time.filter((df_fmt_time.earthquake_datetime > datetime.fromisoformat('1900-01-01')) & (df_fmt_time.earthquake_datetime < datetime.now())).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279725d5-1129-41dd-86df-46176b997869",
   "metadata": {},
   "source": [
    "The filtered dataframe has still the same counts as the original dataframe. The column is also timestamp. Let's now proceed to data cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "15644be3-9ee5-49ad-a50a-9effeb8f365d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_fmt_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "79f40e2a-6dfa-47cd-add2-83201c833756",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = df.withColumnRenamed('lat', 'latitude').withColumnRenamed('long', 'longitude'). \\\n",
    "    select('place', 'earthquake_datetime', 'magnitude', 'latitude', 'longitude', 'depth', 'alert', 'tsunami', 'type'). \\\n",
    "    filter((df.magnitude >= -1) & (df.magnitude <= 10) & (df.magnitude.isNotNull())). \\\n",
    "    filter((df.lat >= -90) & (df.lat <= 90)). \\\n",
    "    filter((df.long >= -180) & (df.long <= 180)). \\\n",
    "    dropDuplicates(subset=['place', 'earthquake_datetime']). \\\n",
    "    na.fill({'depth': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "51ef2f09-936f-47b8-9d38-5fa43ca69e1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "data": {
      "text/plain": [
       "4431443"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6a85de2e-06c2-4626-979e-f1bf7225d235",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 53:==========================================>              (9 + 3) / 12]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+---------+----------------+-----------------+----------------+-----+-------+----------+\n",
      "|               place|earthquake_datetime|magnitude|        latitude|        longitude|           depth|alert|tsunami|      type|\n",
      "+--------------------+-------------------+---------+----------------+-----------------+----------------+-----+-------+----------+\n",
      "|0 km  of The Geys...|2024-10-27 19:39:16|     0.79| 38.779167175293|-122.757331848145| 1.0900000333786| null|      0|earthquake|\n",
      "|0 km  of The Geys...|2024-11-12 02:15:22|     0.36|           38.78|-122.757833333333|            1.36| null|      0|earthquake|\n",
      "|0 km  of The Geys...|2024-12-08 06:36:23|     1.08|38.7799987792969|-122.757667541504|1.36000001430511| null|      0|earthquake|\n",
      "|0 km E of Alamo, ...|2011-01-30 20:50:21|      0.9|           37.85|         -122.027|           4.077| null|      0|earthquake|\n",
      "|0 km E of Aldrans...|1993-03-30 09:48:08|      0.9|          47.249|           11.458|             5.0| null|      0|earthquake|\n",
      "|0 km E of Allyn, ...|1996-05-12 06:30:42|      0.9|      47.3856667|     -122.8238333|          13.365| null|      0|earthquake|\n",
      "|0 km E of Almanor...|2024-11-18 04:22:55|     1.77|40.2183333333333|-121.172166666667|            5.28| null|      0|earthquake|\n",
      "|0 km E of Avard, ...|2021-11-12 06:06:08|      1.2|         36.6995|     -98.77866667|            7.47| null|      0|earthquake|\n",
      "|0 km E of AÃ±asco,...|2019-12-07 09:13:07|     2.67|         18.2841|         -67.1311|            91.0| null|      0|earthquake|\n",
      "|0 km E of Big Lak...|2019-02-25 17:12:50|      0.8|         61.5202|        -149.9383|            41.8| null|      0|earthquake|\n",
      "|0 km E of Big Lak...|2024-11-08 02:27:17|      1.6|         61.5214|        -149.9457|            39.7| null|      0|earthquake|\n",
      "|0 km E of Black D...|1991-08-08 03:03:24|      1.4|      47.3078333|     -121.9928333|          -1.277| null|      0| explosion|\n",
      "|0 km E of Bodelsh...|2005-05-14 09:10:03|      2.5|           48.39|             8.99|            13.0| null|      0|earthquake|\n",
      "|0 km E of Calinga...|1997-03-15 06:46:17|      3.2|         -31.335|          -69.414|           150.0| null|      0|earthquake|\n",
      "|0 km E of Carneli...|2004-01-12 11:08:15|      0.4|          39.227|        -120.0768|            10.5| null|      0|earthquake|\n",
      "|0 km E of Ceriana...|1989-08-31 13:58:25|      2.0|          43.881|            7.778|            10.0| null|      0|earthquake|\n",
      "|0 km E of Chase, ...|2024-10-06 04:49:35|      1.2|         62.4486|        -150.0826|            15.0| null|      0|earthquake|\n",
      "|0 km E of Cidra, ...|2006-06-26 12:35:50|      2.8|          18.176|          -66.152|            19.4| null|      0|earthquake|\n",
      "|0 km E of Claremo...|1988-01-05 22:39:18|      3.3|           38.72|           -87.96|             5.4| null|      0|earthquake|\n",
      "|0 km E of Cloverd...|1990-07-22 11:02:01|      1.6|      38.8066667|     -123.0086667|            3.52| null|      0|earthquake|\n",
      "+--------------------+-------------------+---------+----------------+-----------------+----------------+-----+-------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "df_clean.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d9650282-afa4-49e2-bee8-60354909a3f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- place: string (nullable = true)\n",
      " |-- earthquake_datetime: timestamp (nullable = true)\n",
      " |-- magnitude: double (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- depth: double (nullable = false)\n",
      " |-- alert: string (nullable = true)\n",
      " |-- tsunami: integer (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_clean.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca60c29-9fc8-474c-ae8c-d19fe1f40374",
   "metadata": {},
   "source": [
    "For the cleaning, we have converted the time (milliseconds from the epoch) to a useful and readable datetime format. We have also removed the tz column which has very small non-null values.<br>\n",
    "We have removed null magnitudes and values that are out of range. Same for depth, latitude, and longitude. We have transformed null depth values to 0 and lastly, we dropped duplicate rows base on place and time.<br>\n",
    "We can now stage the data to the warehouse and do some further dbt transformations to create data models.\n",
    "\n",
    "Note: This is just trial cleaning so we can interactively watch the process. Official script is stored in the pipeline folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3ad1c4-7ac1-4501-b47f-37efe5b35258",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
